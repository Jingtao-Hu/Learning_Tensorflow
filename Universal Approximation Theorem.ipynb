{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that any feed forward neural network with a single hidden layer containing a finite number of neurons can fit any function.  \n",
    "\n",
    "Creating a neural networks with one hidden layer correctly classifies all samples in MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/quansun/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n",
      "Shape of the training images:  (55000, 784)\n",
      "Shape of the test images:  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./mnist',one_hot=True)\n",
    "train_images = mnist.train.images\n",
    "test_images = mnist.test.images\n",
    "print(\"Shape of the training images: \", train_images.shape)\n",
    "print(\"Shape of the test images: \", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respectively setting 20 and 1000 hidden neurons, let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0/20000, training accuracy 0.125\n",
      "step 2000/20000, training accuracy 0.90625\n",
      "step 4000/20000, training accuracy 0.96875\n",
      "step 6000/20000, training accuracy 0.9375\n",
      "step 8000/20000, training accuracy 0.875\n",
      "step 10000/20000, training accuracy 0.9375\n",
      "step 12000/20000, training accuracy 0.90625\n",
      "step 14000/20000, training accuracy 0.96875\n",
      "step 16000/20000, training accuracy 0.96875\n",
      "step 18000/20000, training accuracy 1.0\n",
      "Testing accuracy by the neural network with one hidden layer containing 20 neurons: 0.9493\n",
      "Number of digits in training set being correctly classified 9493/10000\n"
     ]
    }
   ],
   "source": [
    "num_of_hidden_neurons = 20\n",
    "num_of_pixels = 28*28\n",
    "num_of_classes = 10\n",
    "epochs = 20000\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32,shape=[None,num_of_pixels])\n",
    "y_ = tf.placeholder(tf.float32,shape=[None,num_of_classes])\n",
    "\n",
    "W1 = tf.get_variable(name='W1',shape=[num_of_pixels,num_of_hidden_neurons],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b1 = tf.get_variable(name='b1',shape=[num_of_hidden_neurons],initializer=tf.constant_initializer(0.1))\n",
    "h1 = tf.matmul(X,W1) + b1\n",
    "h1 = tf.nn.relu(h1)\n",
    "\n",
    "W2 = tf.get_variable(name='W2',shape=[num_of_hidden_neurons,num_of_classes],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b2 = tf.get_variable(name='b2',shape=[num_of_classes],initializer=tf.constant_initializer(0.1))\n",
    "y = tf.matmul(h1,W2) + b2\n",
    "y = tf.nn.relu(y)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y,labels=y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "num_of_correct_prediction = tf.reduce_sum(tf.cast(correct_prediction,tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "    _,training_loss = sess.run([train_step,loss],feed_dict={X:batch_x/255.0,y_:batch_y})\n",
    "    \n",
    "    if epoch%2000 == 0:\n",
    "        training_accuracy = accuracy.eval(session=sess,feed_dict={X:batch_x/255.0,y_:batch_y})\n",
    "        print(\"step {0}/{1}, training accuracy {2}\".format(epoch,epochs,training_accuracy))\n",
    "\n",
    "testing_accuracy = accuracy.eval(session=sess,feed_dict={X:mnist.test.images/255.0,y_:mnist.test.labels})\n",
    "num_of_correct = sess.run(num_of_correct_prediction,feed_dict={X:mnist.test.images/255.0,y_:mnist.test.labels})\n",
    "print(\"Testing accuracy by the neural network with one hidden layer containing 20 neurons: %g\"%(testing_accuracy))\n",
    "print('Number of digits in training set being correctly classified {0}/{1}'.format(num_of_correct,test_images.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0/20000, training accuracy 0.21360759437084198\n",
      "step 2000/20000, training accuracy 0.09810126572847366\n",
      "step 4000/20000, training accuracy 0.1139240488409996\n",
      "step 6000/20000, training accuracy 0.0901898741722107\n",
      "step 8000/20000, training accuracy 0.10601265728473663\n",
      "step 10000/20000, training accuracy 0.10126582533121109\n",
      "step 12000/20000, training accuracy 0.0917721539735794\n",
      "step 14000/20000, training accuracy 0.08227848261594772\n",
      "step 16000/20000, training accuracy 0.10601265728473663\n",
      "step 18000/20000, training accuracy 0.10443037748336792\n",
      "Testing accuracy by the neural network with one hidden layer containing 20 neurons: 0.098\n",
      "Number of digits in training set being correctly classified 980/10000\n"
     ]
    }
   ],
   "source": [
    "num_of_hidden_neurons = 1000\n",
    "num_of_pixels = 28*28\n",
    "num_of_classes = 10\n",
    "epochs = 20000\n",
    "batch_size = 632\n",
    "learning_rate = 0.01\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32,shape=[None,num_of_pixels])\n",
    "y_ = tf.placeholder(tf.float32,shape=[None,num_of_classes])\n",
    "\n",
    "W1 = tf.get_variable(name='W1',shape=[num_of_pixels,num_of_hidden_neurons],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b1 = tf.get_variable(name='b1',shape=[num_of_hidden_neurons],initializer=tf.constant_initializer(0.1))\n",
    "h1 = tf.matmul(X,W1) + b1\n",
    "h1 = tf.nn.relu(h1)\n",
    "\n",
    "W2 = tf.get_variable(name='W2',shape=[num_of_hidden_neurons,num_of_classes],initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b2 = tf.get_variable(name='b2',shape=[num_of_classes],initializer=tf.constant_initializer(0.1))\n",
    "y = tf.matmul(h1,W2) + b2\n",
    "y = tf.nn.relu(y)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y,labels=y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "num_of_correct_prediction = tf.reduce_sum(tf.cast(correct_prediction,tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "    _,training_loss = sess.run([train_step,loss],feed_dict={X:batch_x/255.0,y_:batch_y})\n",
    "    \n",
    "    if epoch%2000 == 0:\n",
    "        training_accuracy = accuracy.eval(session=sess,feed_dict={X:batch_x/255.0,y_:batch_y})\n",
    "        print(\"step {0}/{1}, training accuracy {2}\".format(epoch,epochs,training_accuracy))\n",
    "\n",
    "testing_accuracy = accuracy.eval(session=sess,feed_dict={X:mnist.test.images/255.0,y_:mnist.test.labels})\n",
    "num_of_correct = sess.run(num_of_correct_prediction,feed_dict={X:mnist.test.images/255.0,y_:mnist.test.labels})\n",
    "print(\"Testing accuracy by the neural network with one hidden layer containing 20 neurons: %g\"%(testing_accuracy))\n",
    "print('Number of digits in training set being correctly classified {0}/{1}'.format(num_of_correct,test_images.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "According to our results, we can clearly find that a simple neural networks with one hidden layer can perform well on mnist classification. However, more neurons in hidden layer doesn't mean better performance. We can see the bad performance when the number of hidden layer neurons is up to 1000. So our results mean the universial approximation theorem is right but depends on the different situations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
